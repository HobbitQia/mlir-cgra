{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the following commands to clear output folders\n",
    "# !rm -rf output\n",
    "# !rm -rf model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SODA Toolchain\n",
    "\n",
    "![tutorial-flow](imgs/flow-diagram-full.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-Level Application Input (TensorFlow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a model in tensorflow (Step 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
    "import numpy as np\n",
    "tf.random.set_seed(seed=0)\n",
    "print(tf.__version__)\n",
    "\n",
    "in1 = keras.layers.Input(shape=(32,32,1))\n",
    "tmp = keras.layers.Conv2D(filters=1, kernel_size=(5,5),\n",
    "                          input_shape=(32,32),\n",
    "                          padding='same', \n",
    "                          strides=(2, 2),\n",
    "                          activation='relu', \n",
    "                          use_bias=True)(in1)\n",
    "tmp = keras.layers.Flatten()(tmp)\n",
    "tmp = keras.layers.Dense(units=8, activation='relu')(tmp)\n",
    "tmp = keras.layers.Dense(units=4, activation='relu')(tmp)\n",
    "out = keras.layers.Dense(units=2, activation='softmax')(tmp)\n",
    "model = keras.models.Model(inputs=[in1], outputs=out)\n",
    "\n",
    "# Compile model with optimizer\n",
    "model.compile(optimizer=\"adam\",\n",
    "                loss=\"sparse_categorical_crossentropy\",\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert model to protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(os.getcwd(), \"model/simple/\")\n",
    "\n",
    "# Save model to SavedModel format\n",
    "tf.saved_model.save(model, save_path)\n",
    "\n",
    "# Convert Keras model to ConcreteFunction\n",
    "full_model = tf.function(lambda x: model(x))\n",
    "full_model = full_model.get_concrete_function(\n",
    "    x=[\n",
    "        tf.TensorSpec(model.inputs[0].shape, model.inputs[0].dtype, name='x1')\n",
    "    ])\n",
    "\n",
    "# Get frozen ConcreteFunction\n",
    "frozen_func = convert_variables_to_constants_v2(full_model)\n",
    "\n",
    "# Save frozen graph from frozen ConcreteFunction to hard drive\n",
    "tf.io.write_graph(graph_or_graph_def=frozen_func.graph,\n",
    "                    logdir=os.getcwd(),\n",
    "                    name=\"output/frozen_graph.pbtxt\",\n",
    "                    as_text=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tutorial-flow](imgs/flow-diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform protobuf into MLIR (Step 2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scripts/protobuf-to-tosa.sh output/frozen_graph.pbtxt output/tosa.mlir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower MLIR to Linalg on Buffers (Step 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scripts/tosa-to-linalg.sh output/tosa.mlir output/linalg-buffers.mlir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tutorial-flow](imgs/flow-diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SODA-OPT: HW/SW Partitioning and Optimizer (Step 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use soda.launch?\n",
    "\n",
    "### Automatic selection of custom accelerator region\n",
    "\n",
    "Using the pass: `-convert-<abstraction_name>-<operation_name>-to-soda`\n",
    "\n",
    "Such as: `-convert-linalg-generic-to-soda`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual selection of custom accelerator region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the following lines around any code that will become the accelerator:\n",
    "\n",
    "```\n",
    "soda.launch {\n",
    "  // ...\n",
    "  // Code to be transformed into an accelerator\n",
    "  // ...\n",
    "  soda.terminator\n",
    "}\n",
    "```\n",
    "\n",
    "Run next cell and edit the generated file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp output/linalg-buffers.mlir output/01searched-edited.mlir\n",
    "\n",
    "# Perform manual edit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit the [file](output/01searched-edited.mlir).\n",
    "\n",
    "Around line 99, modify code to look like this:\n",
    "\n",
    "```\n",
    "soda.launch {\n",
    "  linalg.batch_matmul ins(%23, %7 : memref<1x4x8xf32>, memref<1x8x4xf32>) \n",
    "                      outs(%25 : memref<1x4x4xf32>)\n",
    "  soda.terminator\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization pipeline\n",
    "\n",
    "![optimizations](imgs/optimization-table.png)\n",
    "\n",
    "### Kernel without SODA-OPT optimizations (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "(\n",
    "  docker run -u $(id -u) -v $(pwd):/working_dir --rm agostini01/soda \\\n",
    "  soda-opt \\\n",
    "    -soda-outline-bambu-code \\\n",
    "    -soda-extract-arguments-to-xml=using-bare-ptr \\\n",
    "    -soda-generate-bambu-accelcode=no-aa \\\n",
    "    -lower-all-to-llvm=use-bare-ptr-memref-call-conv \\\n",
    "    -mlir-print-ir-after-all \\\n",
    "    output/01searched-edited.mlir \\\n",
    "    -o output/04baseline.mlir \\\n",
    "    2>&1 | cat > output/05intermediate-baseline.mlir\n",
    "\n",
    "  docker run -u $(id -u) -v $(pwd):/working_dir --rm agostini01/soda \\\n",
    "  mlir-translate -opaque-pointers=0  \\\n",
    "    --mlir-to-llvmir \\\n",
    "    output/04baseline.mlir \\\n",
    "    -o output/05baseline.ll\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize [intermediate file](output/05intermediate-baseline.mlir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel with SODA-OPT optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "(\n",
    "  docker run -u $(id -u) -v $(pwd):/working_dir --rm agostini01/soda \\\n",
    "  soda-opt \\\n",
    "    -soda-outline-bambu-code \\\n",
    "    -soda-extract-arguments-to-xml=using-bare-ptr \\\n",
    "    -soda-generate-bambu-accelcode \\\n",
    "    -soda-opt-pipeline-for-bambu=use-bare-ptr-memref-call-conv \\\n",
    "    -mlir-print-ir-after-all \\\n",
    "    output/01searched-edited.mlir \\\n",
    "    -o output/04optimized.mlir \\\n",
    "    2>&1 | cat > output/05intermediate-optimized.mlir\n",
    "\n",
    "  docker run -u $(id -u) -v $(pwd):/working_dir --rm agostini01/soda \\\n",
    "  mlir-translate -opaque-pointers=0  \\\n",
    "    --mlir-to-llvmir \\\n",
    "    output/04optimized.mlir \\\n",
    "    -o output/05optimized.ll\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize [intermediate file](output/05intermediate-optimized.mlir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tutorial-flow](imgs/flow-diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bambu: Synthesizing the Outlined Kernel (Step 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following configurations are passed to our backend HLS tool:\n",
    "\n",
    "* Target: FPGA generation using the Xilinx xc7vx690t-3ffg1930-VVD device\n",
    "* Memory technology: BRAM\n",
    "* Number of memory channels: 2\n",
    "  * Supports 2 parallel reads and 2 parallel writes\n",
    "* Target frequency: 200MHz (5ns period)\n",
    "* Using bambu's floating-point operation support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! scripts/run-bambu.sh baseline\n",
    "# Takes aprox 20 seconds to execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_runtime = \"\"\n",
    "\n",
    "for runtime in open('output/baseline/bambu-log').readlines():\n",
    "    if \"Average execution\" in runtime:\n",
    "        baseline_runtime = [int(s) for s in runtime.split() if s.isdigit()][0]\n",
    "\n",
    "print(\"Average execution in cycles: {}\".format(baseline_runtime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize [Intermediate Dot File](output/baseline/HLS_output/dot/main_kernel/HLS_STGraph.dot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimized kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! scripts/run-bambu.sh optimized\n",
    "# Takes aprox 55 seconds to execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_runtime = \"\"\n",
    "\n",
    "for runtime in open('output/optimized/bambu-log').readlines():\n",
    "    if \"Average execution\" in runtime:\n",
    "        optimized_runtime = [int(s) for s in runtime.split() if s.isdigit()][0]\n",
    "\n",
    "print(\"Average execution in cycles: {}\".format(optimized_runtime))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize [Intermediate Dot File](output/optimized/HLS_output/dot/main_kernel/HLS_STGraph.dot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of runtime results\n",
    "\n",
    "* Display runtime\n",
    "* Display [verilog output file](output/optimized/main_kernel.v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average execution in cycles of Baseline kernel:  {}\".format(baseline_runtime))\n",
    "print(\"Average execution in cycles of Optimized kernel: {}\".format(optimized_runtime))\n",
    "print(\"Speedup: {:.1f}\".format(float(baseline_runtime/optimized_runtime)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commandline interface\n",
    "\n",
    "To visualize all possible paramenters for our optimization passes run:\n",
    "\n",
    "- `soda-opt -h`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "      --soda-opt-pipeline-for-bambu                    \n",
    "        --affine-tile-size=<ulong>                     \n",
    "        --bitwidth-of-index-type=<uint>                \n",
    "        --max-alloc-size-in-bytes=<uint>               \n",
    "        --max-rank-of-allocated-memref=<uint>          \n",
    "        --number-of-full-unrolls=<uint>                \n",
    "        --permutation-map=<uint>                       \n",
    "        --use-bare-ptr-memref-call-conv                \n",
    "        --no-alloca-promotion                          \n",
    "        --no-buffer-trick                              \n",
    "        --no-scalar-replacement                        \n",
    "  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "(\n",
    "  docker run -u $(id -u) -v $(pwd):/working_dir --rm agostini01/soda \\\n",
    "  soda-opt -h 2>&1 | cat > output/helpfile\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open [help file](output/helpfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying the number of unrolls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "(\n",
    "  docker run -u $(id -u) -v $(pwd):/working_dir --rm agostini01/soda \\\n",
    "  soda-opt \\\n",
    "    -soda-outline-bambu-code \\\n",
    "    -soda-extract-arguments-to-xml=using-bare-ptr \\\n",
    "    -soda-generate-bambu-accelcode \\\n",
    "    -soda-opt-pipeline-for-bambu=\"use-bare-ptr-memref-call-conv number-of-full-unrolls=1\" \\\n",
    "    -mlir-print-ir-after-all \\\n",
    "    output/01searched-edited.mlir \\\n",
    "    -o output/04optimized.mlir \\\n",
    "    2>&1 | cat > output/05intermediate-optimized.mlir\n",
    "\n",
    "  docker run -u $(id -u) -v $(pwd):/working_dir --rm agostini01/soda \\\n",
    "  mlir-translate -opaque-pointers=0  \\\n",
    "    --mlir-to-llvmir \\\n",
    "    output/04optimized.mlir \\\n",
    "    -o output/05optimized.ll\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize [intermediate file](output/05intermediate-optimized.mlir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! scripts/run-bambu.sh optimized\n",
    "# 57 Seconds to execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_runtime = \"\"\n",
    "\n",
    "for runtime in open('output/optimized/bambu-log').readlines():\n",
    "    if \"Average execution\" in runtime:\n",
    "        optimized_runtime = [int(s) for s in runtime.split() if s.isdigit()][0]\n",
    "\n",
    "print(\"Average execution in cycles: {}\".format(optimized_runtime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tutorial-flow](imgs/flow-diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vivado Flow: Place and Route generated Verilog (Step 6)\n",
    "\n",
    "To successfully execute the foolowing steps, Bambu and Vivado 2020.2 are exptected to be installed **locally**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scripts/run-synthesis.sh <baseline|optimized> <path_to_vivado_installation>\n",
    "! scripts/run-synthesis.sh baseline /files0/Xilinx/Vivado/2020.2\n",
    "\n",
    "# Approx. 4min to execute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimized kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! scripts/run-synthesis.sh optimized /files0/Xilinx/Vivado/2020.2\n",
    "\n",
    "# Approx. 11min to execute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of synthesis results\n",
    "\n",
    "* Cycles\n",
    "* LUTs\n",
    "* DSPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "experiment_id=1\n",
    "rows = ['CYCLES', 'AREAxTIME', 'AREA',\n",
    "        'SLICE', 'SLICE_LUTS', 'REGISTERS',\n",
    "        'DSPS', 'BRAMS', 'PERIOD',\n",
    "        'CLOCK_SLACK', 'FREQUENCY', 'TIME',\n",
    "        'TOTAL_TIME', 'TOTAL_CYCLES', 'HLS_execution_time']\n",
    "df_baseline = pd.read_xml('output/baseline/bambu_results_{}.xml'.format(experiment_id), names=['baseline'])\n",
    "df_optimized = pd.read_xml('output/optimized/bambu_results_{}.xml'.format(experiment_id), names=['optimized'])\n",
    "df = pd.concat([df_baseline, df_optimized], axis=1)\n",
    "df.index=rows\n",
    "df['optimized/baseline']=df['optimized']/df['baseline']\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post place and route comparison\n",
    "\n",
    "Considering a matrix multiply kernel has approximatelly 2xNxMxK arithmetic operations\n",
    "\n",
    "And our selected kernel has the following sizes: \n",
    "\n",
    "```\n",
    "linalg.batch_matmul ins(%23, %6 : memref<1x4x8xf32>, memref<1x8x4xf32>) \n",
    "                    outs(%25 : memref<1x4x4xf32>)\n",
    "```\n",
    "M=4, K=8, N=4\n",
    "\n",
    "We have approximatelly **256** floating point aritihmetic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mega_multiplier=1e6\n",
    "flop_count = 256 # arithmetic float point operations\n",
    "target_frequency = 200e+6 # 200MHz\n",
    "\n",
    "baseline_utilization_area=df.loc['AREA','baseline']\n",
    "optimized_utilization_area=df.loc['AREA','optimized']\n",
    "\n",
    "\n",
    "baseline_runtime_in_s = baseline_runtime/target_frequency \n",
    "optimized_runtime_in_s = optimized_runtime/target_frequency\n",
    "\n",
    "baseline_flops= flop_count/baseline_runtime_in_s\n",
    "optimized_flops= flop_count/optimized_runtime_in_s\n",
    "\n",
    "\n",
    "print(\"Execution in cycles of Baseline kernel:  {}\".format(baseline_runtime))\n",
    "print(\"Execution in cycles of Optimized kernel:   {}\".format(optimized_runtime))\n",
    "\n",
    "print(\"Speedup: \\t\\t\\t{:.2f}x\".format(baseline_runtime/optimized_runtime))\n",
    "print(\"Area utilization overhead: \\t {:.2f}x\".format(optimized_utilization_area/baseline_utilization_area))\n",
    "\n",
    "print(\"Baseline  \\t\\t\\t {:.2f} MFLOPS \".format(baseline_flops/mega_multiplier))\n",
    "print(\"Optimized \\t\\t\\t{:.2f} MFLOPS\".format(optimized_flops/mega_multiplier))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generated Design files\n",
    "\n",
    "Vivado checkpoints can be found here:\n",
    "\n",
    "* output/baseline/HLS_output/Synthesis/vivado_flow_0/post_place.dcp\n",
    "* output/optimized/HLS_output/Synthesis/vivado_flow_0/post_place.dcp\n",
    "* output/output/<baseline|optimized>/HLS_output/Synthesis/vivado_flow_X/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline and Optimized Side by Side\n",
    "\n",
    "<p float=\"middle\">\n",
    "  <img src=\"imgs/baseline_view.png\" width=\"49%\" />\n",
    "  <img src=\"imgs/optimized_view.png\" width=\"49%\" /> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you!"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "93c7c3955e7e5d34b1892eb3eed2f106b954557791a7d399a089a63ede089a6c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
